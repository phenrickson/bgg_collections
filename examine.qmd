---
title: "Predicting BGG Collections"
format: 
        html:
                code-fold: true
                code-overflow: scroll
                code-summary: 'Show the code'
                self-contained: true
                toc: true
                fig-align: center
                theme: cerulean
                message: false
css: styles.css
editor: source
params: 
        username:
                phenrickson
---

```{r}
options(knitr.duplicate.label = "allow")
```


```{r}
#| echo: false
#| include: false
library(targets)
library(dplyr)
library(tibble)
library(gt)
library(gtExtras)
library(ggrepel)
library(tibble)
library(dplyr)
library(rsample)
library(tidymodels)
library(bggUtils)

```

# About

This report details the results of training and evaluating a classification model for predicting games for a user's boardgame collection.

I start by detailing the data involved, the objective of the model, and the resulting models trained for a given user. I then proceed to exploring what the models learned from the data; what predicts a user's collection?

I then detail the different means by which I assess the performance of classification models for the purpose of this project. I assess models based on their ability to provide lift in predicting games, the calibration of their probabilities, and their performance as classifiers.

I then detail the predictions from the model.

```{r}
#| include: false
#| echo: false
# load in dependencies from targets
tar_source("src/data/load_data.R")
tar_source("src/models/splitting.R")
tar_source("src/models/training.R")
tar_source("src/visualization/inference.R")
tar_source("src/visualization/tables.R")
tar_source("src/visualization/plots.R")

```

```{r}
#| echo: false
tar_load(games_raw)
tar_load(games)
username = params$username

tar_load(paste("collection", username, sep = "_"))
tar_load(paste("model_glmnet", username, sep = "_"))
tar_load(paste("model_lightgbm", username, sep = "_"))

collection = 
        get(ls(pattern = paste("collection", username, sep = "_")))

model_glmnet = 
        get(ls(pattern = paste("model_glmnet", username, sep = "_")))

model_lightgbm = 
        get(ls(pattern = paste("model_lightgbm", username, sep = "_")))

rm(list = ls(pattern = paste(params$username)))

preds =   
        bind_rows(
                model_glmnet |>
                        gather_predictions() |>
                        select(username, wflow_id, preds),
                model_lightgbm |>
                        gather_predictions() |>
                        select(username, wflow_id, preds)
        ) |>
        unnest(preds) |>
        mutate(type = factor(type, levels = c("resamples", "valid", "test"))) |>
        nest(preds = -c(username, wflow_id))

metrics = 
        preds |>
        assess_predictions()

theme_set(
        bggUtils::theme_bgg()
)

```

# Data

The data in this project comes from BoardGameGeek.com. The data used is at the game level, where an individual observation contains *features* about a game, such as its publisher, categories, and playing time, among many others. 

<!-- The goal of this project is to evaluate and recommend new board games to a user based on the games that appear in their collection. The outcome variable, *own*, is at the user level and is a binary variable indicating whether an individual user has a game in their collection or not.  -->


## Outcome

I train a classification model at the user level to learn the relationship between game features and games that a user owns - what predicts a user's collection? 

I evaluate evaluate the model's performance on a training set of historical games via resampling, then validate the model's performance on a set aside set of newer relases. I then refit the model on the training and validation in order and predict upcoming releases in order to find new games that the user is most likely to add to their collection.

```{r}

model_glmnet |> 
        pluck("split", 1) |>
        bind_rows(.id = 'data') |>
        # group_by(username, data ) |>
        # mutate(years = paste(min(yearpublished), max(yearpublished), sep = "-")) |>
        group_by(username, data, own) |> 
        count() |>
        pivot_wider(names_from = c("own"),
                    values_from = c("n")) |>
        ungroup() |>
        mutate(data = factor(data, levels = c("test", "valid", "train"))) |>
        arrange(desc(yes)) |>
        gt::gt() |>
        gt::as_raw_html()
```


## Features

```{r}
#| include: false
#| message: false
predictors =
        model_lightgbm |>
        pluck("wflow", 1) |>
        extract_mold() |>
        pluck("predictors")

outcome =
        model_lightgbm |>
        pluck("wflow", 1) |>
        extract_mold() |>
        pluck("outcomes")

ids =
        model_lightgbm |>
        pluck("wflow", 1) |>
        extract_mold() |>
        pluck("extras") |>
        pluck("roles") |>
        pluck("id")


```

I make use of many potential features for games (around `r ncol(predictors)`), the vast majority of which are dummies indicating the presence or absence of the presence or absence of things such as a publisher/artist/designer. The "standard" BGG features for every game contain information that is typically listed on the box its playing time, player counts, or its recommended minimum age.

```{r}
#| message: false
#| warning: false
predictors |>
        select(-starts_with("mechanics"),
               -starts_with("categories"),
               -starts_with("families"),
               -starts_with("publishers"),
               -starts_with("designers"),
               -starts_with("artists"),
               -starts_with("components"),
               -starts_with("themes"),
               -starts_with("mechanisms"),
               -starts_with("missing"),
               -starts_with("published_before_1900")) |>
        corrr::correlate() |>
        corrr::rearrange() |>
        autoplot()

```

<!-- For instance, a game such as Ticket to Ride has data that appears in the following way. -->

<!-- ```{r} -->

<!-- games |> -->
<!--         filter(name == 'Ticket to Ride') |> -->
<!--         select(game_id, name, yearpublished, minplayers, maxplayers, playingtime, minage, publishers, designers, categories, mechanics, components) |> -->
<!--         gt::gt() -->

<!-- ``` -->


<!-- I preprocess the data for each game using a recipe, creating dummies for the categorical variables. -->

<!-- ```{r} -->

<!-- bind_cols(predictors, ids) |> -->
<!--         filter(game_id == 9209) |> -->
<!--         select( -->
<!--                  game_id, name, yearpublished, year, minplayers, maxplayers, playingtime, minage, time_per_player,  -->
<!--                  starts_with("mechanics"), -->
<!--                  starts_with("designers"), -->
<!--                  starts_with("publishers"), -->
<!--                  starts_with("categories") -->
<!--         ) |> -->
<!--         filter(across()) -->


<!-- ``` -->


# Modeling

I’ll now the examine predictive models trained on the user’s collection.

For an individual user, I train a predictive model on their collection in order to predict whether a user owns a game. The outcome, in this case, is binary: does the user have a game listed in their collection or not? This is the setting for training a classification model, where the model aims to learn the probability that a user will add a game to their collection based on its observable features.

How does a model learn what a user is likely to own? The training process is a matter of examining historical games and finding patterns that exist between game features (designers, mechanics, playing time, etc) and games in the user’s collection.

<!-- ::: {.callout-note} -->

<!-- I train models to predict whether a user owns a game based only on information that could be observed about the game at its release: playing time, player count, mechanics, categories, genres, and selected designers, artists, and publishers. I do not make use of BGG community information, such as its average rating, weight, or number of user ratings. This is to ensure the model can predict newly released games without relying on information from the BGG community. -->

<!-- ::: -->

## Inference

A predictive model gives us more than just predictions. We can also ask, what did the model learn from the data? What predicts the outcome? In the case of predicting a boardgame collection, what did the model find to be predictive of games a user has in their collection?

I trained and evaluated two types of models: a (penalized) logistic regression and boosted trees (with LightGBM). 

First, I examine the coefficients from a penalized logistic regression with ridge regularization (which I will refer to as a penalized logistic regression).

Positive values indicate that a feature increases a user’s probability of owning/rating a game, while negative values indicate a feature decreases the probability. To be precise, the coefficients indicate the effect of a particular feature on the log-odds of a user owning a game.

```{r}
#| include: false
#| message: false
#| warning: false
top_coefs_plot = 
        model_glmnet |> 
        pluck("wflow", 1) |>
        get_coefs.glmnet() |>
        top_coefs_by_sign() |>
        coef_plot.glmnet()

```


```{r}
#| fig-height: 6
top_coefs_plot+
        facet_wrap(~params$username)

```

The following visualization shows the path of each feature as it enters the model, with highly influential features tending to enter the model early with large positive or negative effects. The dotted line indicates the level of regularization that was selected during tuning.

```{r}
#| warning: false
#| message: false
#| 
model_glmnet |>
        pluck("wflow", 1) |>
        trace_plot.glmnet(max.overlaps = 30)+
        facet_wrap(~params$username)

```

## Variable Importance

Which features were found to be predictive of the user's collection? The following plot displays the most important features from the boosted trees model, based on three model-specific metrics.

Gain is the contribution each feature made to the total gain from splits involving the feature; cover is the number of observations related to a feature; frequency is the percentage of times each feature was used by each tree.

```{r}
#| fig-height: 6
lightgbm = 
        model_lightgbm |>
        pluck("wflow", 1) |>
        extract_fit_engine() |>
        lightgbm::lgb.restore_handle()

vi = 
        map_df(
                c('frequency', 'cover', 'gain'),
                ~ lightgbm |>
                        vip::vi_model(.x,
                                      percentage = T) |>
                        mutate(type = .x)
        )

vi |> 
        group_by(type) |> 
        slice_max(Importance, n = 30) |> 
        mutate(Variable = bggUtils::present_bgg_text(Variable, minlength = 35)) |> 
        ggplot(aes(x=Importance, 
                   y = reorder(Variable, Importance)))+
        #          y=tidytext::reorder_within(Variable, Importance, type)))+
        geom_col()+
        facet_wrap(~type, scales = "free_x", ncol = 3) + 
        tidytext::scale_y_reordered()+
        ylab("Feature")

```

## Partial Effects

What were the effects of different types of features? The following plots display the coefficients from the logistic regression for different subsets of features.

Use the buttons below to examine the effects different types of predictors had in predicting the user's collection.

```{r}
#| echo: false
#| message: false
#| warning: false
coefs =
        model_glmnet |>
        pluck("wflow", 1) |>
        get_coefs.glmnet()

groups = c('Mechanics', 'Designers', 'Artists', 'Publishers', 'Categories', 'Families', 'Themes', 'Components')

plots =
        map(
                tolower(groups),
                ~ coefs |>
                        mutate(term = gsub("themes_theme_", "themes_", term)) |>
                        coef_plot_by_group(group = .x,
                                           shrink = F,
                                           scales = "free")+
                        ggh4x::facet_grid2(
                                sign~.,
                                scales = "free_y"
                        )+
                        geom_vline(xintercept = c(min(coefs$estimate),
                                                  max(coefs$estimate)),
                                   linetype = 'dotted',
                                   alpha = 0.25)+
                        labs(
                                subtitle = ''
                        )
        )

names(plots) = groups

```

::: {.panel-tabset .nav-pills}

### Categories

```{r}
#| echo: false
#| class: scroll
#| fig-height: 6
plots$Categories

```

### Mechanics

```{r}
#| echo: false
#| class: scroll
#| fig-height: 6
plots$Mechanics

```

### Themes

```{r}
#| echo: false
#| class: scroll
#| fig-height: 6
plots$Themes

```

### Families

```{r}
#| echo: false
#| class: scroll
#| fig-height: 6
plots$Families

```

### Publishers

```{r}
#| echo: false
#| class: scroll
#| fig-height: 6
plots$Publishers

```

### Designers

```{r}
#| echo: false
#| class: scroll
#| fig-height: 6
plots$Designers
```

### Artists

```{r}
#| echo: false
#| class: scroll
#| fig-height: 6
plots$Artists

```

### Components

```{r}
#| echo: false
#| class: scroll
#| fig-height: 6
plots$Components

```

:::

# Assessment

How well did the model(s) do in predicting the user’s collection? This section contains a variety of visualizations and metrics for assessing the performance of the model(s).

Given that the outcome is a rare event, I am interested in evaluating **probabilibities** produced by the models moreso than **classifications**, so I principally evaluate models based on metrics such as the log loss, the area under the receiver operating characteristic (ROC) curve, and the area under the precision-recall (PR) curve.

The following displays the model's performance in resampling on a training set, a validation set, and a holdout set (test) of upcoming games.

```{r}

metrics |>
        pivot_wider(names_from = c("wflow_id"),
                    values_from = c(".estimate")) |>
        group_by(username, type) |>
        gt::gt() |>
        gt::fmt_number(
                columns = everything(),
                decimals = 3
        ) |>
        gt::as_raw_html()

```

Both models exhibit similar performance overall, with the penalized regression (glmnet) slightly outperforming lightgbm on the validation set based on area under the ROC/PR curves.

## Separation

An easy way to visually examine the performance of classification model is to view a separation plot.

I plot the predicted probabilities from the model for every game (during resampling) from lowest to highest. I then overlay a blue line for any game that the user does own. A good classifier is one that is able to separate the blue (games owned by the user) from the white (games not owned by the user), with most of the blue occurring at the highest probabilities (right side of the chart).

```{r}

plot_separation = 
        function(preds) {
                
                ranks = 
                        preds |>
                        arrange(.pred_yes) |>
                        mutate(rank = row_number())
                
                ranks |>
                        ggplot(aes(x=rank,
                                   y=.pred_yes))+
                        geom_point(size = 0.25,
                                   alpha = 0.25) +
                        geom_vline(data = ranks |>
                                           filter(own =='yes'),
                                   aes(xintercept = rank),
                                   color = 'deepskyblue1',
                                   alpha = 0.25
                        )+
                        facet_grid(wflow_id ~ type)
        }

preds |>
        unnest(preds) |>
        filter(type %in% c('resamples', 'valid')) |>
        plot_separation()

```

I can more formally assess how well each model did in resampling by looking at the area under the ROC curve (roc_auc). A perfect model would receive a score of 1, while a model that cannot predict the outcome will default to a score of 0.5. The extent to which something is a good score depends on the setting, but generally anything in the .8 to .9 range is very good while the .7 to .8 range is perfectly acceptable.

```{r}
#| warning: false
#| message: false
preds |>
        unnest() |>
        filter(type %in% c('valid', 'resamples')) |>
        group_by(wflow_id, type) |>
        yardstick::roc_curve(
                own,
                .pred_yes,
                event_level = 'second'
        ) |>
        ggplot(aes(x=1-specificity,
                   color = wflow_id,
                   y=sensitivity))+
        geom_line()+
        bggUtils::theme_bgg()+
        geom_abline(slope = 1,
                    linetype = 'dotted')+
        ggthemes::scale_color_colorblind()+
        facet_wrap(~type,
                   ncol = 2)

```

### Gain

We can similarly look at a gain curve to get a sense of how quickly a model is able to help us detect instances of the outcome. One way to think about this: if we were to sort the top 10% games ranked by our model, what percentage of games that the user owns would we find? If we were to pick games at random, we would expect to find 10% of games in the user's collection. But, if our classifier is performing well, we would hope to find many more games with the outcome than we would by random chance. This difference is the lift, or gain, provided by a model over random selection.

In the case of these models, searching the top 10% of the model's predictions allows us to find nearly over 90% of games owned by the user during resampling, and 96% of games in the validation set.

```{r}
#| message: false
#| warning: false
gain = 
        preds |>
        unnest(preds) |>
        filter(type %in% c('valid', 'resamples')) |>
        group_by(wflow_id, type) |>
        yardstick::gain_curve(
                own,
                .pred_yes,
                event_level = 'second'
        )
        
gain |>
        filter(round(.percent_tested, 1) == 10) |>
        group_by(wflow_id, type) |>
        group_by(type) |>
        slice_max(.percent_tested, n =1) |>
        gt::gt() |>
        gt::fmt_number(
                columns = c(".n_events", ".percent_tested", ".percent_found"),
                decimals = 3)

```

The following table provides the full gain curve for each model.

```{r}

#| message: false
#| warning: false
gain |>
        group_by(wflow_id, type) |>
        slice(which(row_number() %% 100 == 1)) |>
        ungroup() |>
        gt::gt() |>
        gt::fmt_number(
                columns = c(".n_events", ".percent_tested", ".percent_found"),
                decimals = 3
        ) |>
        gt::opt_interactive()
        

```


## Classification

One downside to relying on the area under the ROC curve is that it can lead to inflated expectations of model performance with a very imbalanced outcome, when there are many instances of the negative class (in this case, games the user does not own). A precision-recall curve, in contrast, focuses on the positive class and can be more informative regarding the model's ability to balance false positives vs false negatives when predicting whether a rare event happens.

### Precision-Recall

To see this, I'll the plot the model's precision and recall as a function of different classification thresholds. Precision and recall are both metrics that assess a model's classifications - whether it correctly predicted 'yes' or 'no' - instead of probabilities. 

Precision refers to the rate at which the model was correct when the model predicted that the outcome would occur: true positives / (true positives + false positives). In this setting, precision tells us: when the model predicts that the user owns a game, how often was it correct?

Recall refers to the rate at which the model was able to find when the outcome occurred: true positives / (true positives + false negatives). In the case of a boardgame collection, recall tells us how many games in a user's collection that the model was able to correctly classify. 

In the following plot, I let the classification threshold range from 0 to 1 and plot the precision/recall at each possible threshold.

```{r}
#| message: false
#| warning: false

thresholds = seq(0, 1, by = 0.005)

class_metrics = metric_set(yardstick::bal_accuracy, mcc, kap, accuracy, precision, recall, f_meas)

results = 
        map_df(
                thresholds,
                ~   preds |>
                        unnest(preds) |>
                        filter(type %in% c('resamples', 'valid')) |>
                        mutate(.pred_class = case_when(.pred_yes >= .x ~ 'yes',
                                                       TRUE ~ 'no'),
                               .pred_class = factor(.pred_class, levels = c("no", "yes"))) |>
                        mutate(threshold = .x) |>
                        group_by(wflow_id, type, threshold) |>
                        class_metrics(
                                truth = own,
                                estimate =  .pred_class,
                                event_level = 'second'
                        )
        )

```

```{r}

plot_result = function(results,
                       metrics = c("precision", "recall"),
                       ncol = 1) {
        
        results |>
                filter(.metric %in% metrics) |>
                ggplot(aes(x=threshold,
                           y=.estimate,
                           color = wflow_id))+
                geom_line()+
                facet_wrap(type + .metric ~.,
                           ncol = ncol)+
                ggthemes::scale_color_colorblind()
}

results |>
        filter(type %in% c('resamples')) |>
        plot_result()

```

This illustrates the inherent tradeoff between precision and recall. When the classification threshold is low, the model's recall is very high, as almost every game is predicted to be owned by the user. But, at the same time this means the precision is low, because most of those predictions are false positives - the user doesn't own that many games we predicted they would own.

As the classification threshold moves higher, the recall starts to drop - the model becomes more selective in the games it predicts the user will own, meaning we start to miss some games they own - while the precision starts to improve as we predict only games with a high probability.

The precision-recall curve shows us this information in one plot, which is informative for making use of the model's predictions. The area under this curve tells us about the model's ability to improve over a baseline -  what is a good number for area under the precision recall curve? The floor is the observed proportion of the outcome, in this case close to 0.004.

```{r}

preds |>
        unnest(preds) |>
        filter(type %in% c('resamples', 'valid')) |>
        group_by(wflow_id, type) |>
        yardstick::pr_curve(own, .pred_yes, event_level = 'second') |>
        ggplot(aes(x=recall, y=precision, color = wflow_id))+
        geom_line()+
        ggthemes::scale_color_colorblind()+
        facet_wrap(~type)

```

Viewing the area under the precision-recall curve in addition to the area under the ROC curve tells us that while our models generally do a good job in separating games the user owns from those they do not, our model will not always *hit* on its predictions - we struggle to be precise in our predictions without missing games that the user owns.

For this particular setting, which should we value more, precision or recall? There's an argument to be made that precision is less important than recall, as we want to capture games the user might be interested in owning and we'd be okay with a fair number of false positives in exchange for finding games that are a good fit.

### Thresholds

If we had to set a classification threshold for a model, what should it be? This choice depends on what we're ultimately trying to achieve with the model. Different thresholds maximize different classification metrics, and we would estimate the best threshold for each metric based on performance in resampling.

```{r}

best_thresh = 
        results |>
        filter(type == 'resamples') |>
        filter(.metric %in% c('bal_accuracy', 'f_meas', 'mcc', 'precision')) |>
        group_by(wflow_id, type, .metric) |>
        slice_max(.estimate, n= 1, with_ties = F)

best_thresh |>
        group_by(wflow_id) |>
        gt::gt() |>
        gt::fmt_number(decimals = 3) |>
        gt::as_raw_html()

```
We would then take these thresholds and apply them to the validation set.

If we want to have a very precise model (with low recall), we would set the threshold high, close to 0.655 or 0.7 for each model. This would give us very few predictions for games that the user owns, but these predictions would generally be correct.

```{r}

conf_mats = preds |>
        unnest(preds) |>
        filter(type == 'valid') |>
        select(username, wflow_id, type, .pred_yes, own) |>
        inner_join(
                best_thresh |>
                        ungroup() |>
                        select(wflow_id, threshold, .metric, .estimate),
                relationship = "many-to-many",
                by = join_by(wflow_id)
        ) |>
        mutate(.pred_class = case_when(.pred_yes > threshold ~ 'yes',
                                       TRUE ~ 'no'),
               .pred_class = factor(.pred_class, levels = c("no", "yes"))) |>
        nest(data = -c(username, wflow_id, type, .metric, threshold)) |>
        mutate(conf = map(data, ~ .x |> conf_mat(own, .pred_class))) |>
        mutate(plot = map2(conf, paste(wflow_id, .metric, round(threshold, 4), sep = "\n"),  ~ .x |> autoplot(type = 'heatmap')+ggtitle(.y)))


conf_mats |> 
        filter(.metric == 'precision') |>
        pull(plot) |>
        walk(print)

```

If we wanted to balance precision-recall, we could set the threshold to maximize the F1 measure, which is the harmonic mean of precision and recall. Maximizing the F1 measure will lower the threshold and produce more false positives in exchange for fewer false negatives.

```{r}

conf_mats |> 
        filter(.metric == 'f_meas') |>
        pull(plot) |>
        walk(print)

```

Maximizing something like the balanced accuracy, which is the mean of sensitivity + specificity, will lead to a low threshold and many false positives in exchange for a low number of false negatives.

```{r}

conf_mats |> 
        filter(.metric == 'bal_accuracy') |>
        pull(plot) |>
        walk(print)

```


## Calibration

What do the model’s predicted probabilties mean? Or, put another way, how well calibrated are the model’s predictions?

If the model assigns a probability of 5%, how often does the outcome actually occur? A well calibrated model is one in which the predicted probabilities reflect the probabilities we would observe in the actual data. We can assess the calibration of a model by grouping its predictions into bins and assessing how often we observe the outcome versus how often each model expects to observe the outcome.

A model that is well calibrated will closely follow the dashed line - its expected probabilities match that of the observed probabilities. A model that consistently underestimates the probability of the event will be over this dashed line, be a while a model that overestimates the probability will be under the dashed line.

```{r}

preds |>
        unnest(preds) |>
        filter(type %in% c('resamples', 'valid')) |>
        group_by(wflow_id, type) |>
        probably::.cal_table_windowed(
                #       probably::cal_plot_windowed(
                truth = own,
                estimate = .pred_yes,
                event_level = 'second'
        ) |>
        ggplot(aes(x=predicted_midpoint,,
                   y=event_rate,
                   color = wflow_id))+
        geom_point(aes(size = total),
                   alpha = 0.6)+
        geom_line(alpha= 0.6)+
        facet_wrap(~type,
                   ncol = 2)+
        geom_abline(linetype = 'dashed')+
        ggthemes::scale_color_colorblind()+
        guides(size = 'none')+
        xlab("Predicted Probability")+
        ylab("Observed Probability")

```

## Subpopulations

I'll now evaluate the models by different subpopulations to examine if they have systematically better/worse performance for different types of games.

### Year

How well do the models perform across different years? I'll evaluate each model's performance over a subset of years; ideally, a model will exhibit similar performance across all years.

```{r}
#| message: false
#| warning: false
preds |>
        unnest(preds) |>
        filter(yearpublished >= 2000) |>
        group_by(username, wflow_id, yearpublished) |>
        mutate(n = n()) |>
        group_by(username, wflow_id, yearpublished, n) |>
        yardstick::roc_auc(
                own,
                .pred_yes,
                event_level = 'second'
        ) |>
        ggplot(aes(x=yearpublished,
                   y=.estimate,
                   color = wflow_id))+
        geom_point(aes(size = n))+
        geom_line()+
        ggthemes::scale_color_colorblind()

```

### Categories

How well do the models perform for different types of games?

```{r}
#| message: false
#| warning: false

tune_metric_set = tune_metrics()

categories =
        games_raw |>
        select(game_id, links) |>
        bggUtils:::unnest_categories() |>
        select(game_id, type, value) |>
        mutate(value = forcats::fct_lump_n(value, n = 10))

preds |>
        unnest(preds) |>
        filter(type %in% c('resamples')) |>
        select(type, username, wflow_id, .pred_yes, own, game_id, name) |>
        left_join(
                categories |>
                        select(-type),
                by = c("game_id"),
                relationship = "many-to-many"
        ) |>
        distinct() |>
        mutate(value = replace_na(as.character(value), 'None')) |>
        group_by(type, username, wflow_id, value) |>
        tune_metric_set(
                own,
                .pred_yes,
                event_level = 'second'
        ) |>
        ggplot(aes(x=.estimate,
                   y=value,
                   color = wflow_id))+
        geom_point()+
        facet_wrap(~type + .metric,
                   scales = "free_x")+
        ggthemes::scale_color_colorblind()
```

### Popularity

One thing I'm curious about is the model's performance in predicting popular games vs games that are relatively unknown. In predicting new games, the model will not know in advance which games are actually going to be rated by the BGG community, or even those that will be available to purchase. But in general, only a small percentage of games actually attract enough attention to be widely available for purchase. 

It could then be the case that the model is achieving good results simply because it manages to filter out obscure games that will never in practice be considered by the user. If we restrict the model to only consider games that are popular, how well does it do? Does performance drop off a cliff?

The following plot shows model performance as a function of user ratings - I restrict to only games that have at least $n$ number of ratings and assess performance.

```{r}

ratings = c(5, 25, 50, 100, 500, 1000)

results_by_ratings = 
        map_df(ratings,
               ~  preds |>
                       unnest(preds) |>
                       filter(usersrated >= .x) |>
                       mutate(rated = .x) |>
                       group_by(username, wflow_id, type, rated) |>
                       mutate(n = n()) |>
                       group_by(username, wflow_id, type, rated, n) |>
                       tune_metric_set(
                               own,
                               .pred_yes,
                               event_level = 'second'
                       ) |>
                       filter(type == 'valid') |>
                       select(-.estimator)
        )

results_by_ratings |>
        filter(.metric %in% c('mn_log_loss', 'roc_auc')) |>
        ggplot(aes(x=rated,
                   y=.estimate,
                   color = wflow_id))+
        geom_point(aes(size = n))+
        geom_line()+
        scale_x_log10()+
        ggthemes::scale_color_colorblind()+
        facet_wrap(~.metric,
                   ncol = 2,
                   scales = "free_y")+
        geom_hline(yintercept = 0.5,
                   linetype = 'dotted')

```

In this case, we do see the performance of both models decrease as we filter out games that are relatively obscure and unknown, though based on the area under the ROC both are still performing reasonably well as classifiers.

# Predictions

Based on the totality of the results above, I opt to use the penalized logistic regression model as the model in predicting games for this user.

## Top Games in Training

What were the top games from resampling the training set?

```{r}
#| class: scroll

preds |>
        unnest(preds) |>
        filter(type %in% c('resamples')) |>
        filter(wflow_id == 'glmnet') |>
        slice_max(.pred_yes, n=500) |>
        prep_predictions_datatable(
                games = games
        ) |>
        predictions_datatable()
```

## Top Games by Year

Displaying the model's top games for individual years in recent years.

```{r}
#| warning: false
#| message: false
#| class: scroll

preds |>
        unnest(preds) |>
        filter(type %in% c('valid', 'resamples')) |>
        filter(wflow_id %in% 'glmnet') |>
        top_n_preds(
                games = games,
                top_n = 15
        ) |>
        gt_top_n(
                collection = collection |>
                        prep_collection()
        ) |>
        gt::opt_row_striping(row_striping = F)
```

## Validation

What were the top games from the validation set?

```{r}
#| class: scroll

preds |>
        filter(wflow_id %in% c('glmnet')) |>
        unnest(preds) |>
        filter(type == 'valid') |>
        slice_max(.pred_yes, n=500) |>
        prep_predictions_datatable(
                games = games
        ) |>
        predictions_datatable()
```

## Upcoming

What were the model's top predictions for upcoming games?

```{r}
#| class: scroll
#| message: false
#| warning: false

preds |>
        filter(wflow_id %in% c('glmnet')) |>
        unnest(preds) |>
        filter(usersrated >= 5) |>
        prep_predictions_datatable(
                games = games
        ) |>
        predictions_datatable()
```